# -*- coding: utf-8 -*-
# Licensed under a 3-clause BSD style license - see LICENSE.rst
#
# Astropy documentation build configuration file.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this file.
#
# All configuration values have a default. Some values are defined in
# the global Astropy configuration which is loaded here before anything else.
# See astropy.sphinx.conf for which values are set there.

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
# sys.path.insert(0, os.path.abspath('..'))
# IMPORTANT: the above commented section was generated by sphinx-quickstart, but
# is *NOT* appropriate for astropy or Astropy affiliated packages. It is left
# commented out with this explanation to make it clear why this should not be
# done. If the sys.path entry above is added, when the astropy.sphinx.conf
# import occurs, it will import the *source* version of astropy instead of the
# version installed (if invoked as "make html" or directly with sphinx), or the
# version in the build directory (if "python setup.py build_sphinx" is used).
# Thus, any C-extensions that are needed to build the documentation will *not*
# be accessible, and the documentation will not build correctly.

import datetime
import os
import subprocess
import sys

from docutils import nodes
from sphinx.util.docutils import SphinxDirective

from jdaviz import __version__

try:
    from sphinx_astropy.conf.v2 import *  # noqa
except ImportError:
    print('ERROR: the documentation requires the sphinx-astropy package to be installed')
    sys.exit(1)

# -- General configuration ----------------------------------------------------

# By default, highlight as Python 3.
highlight_language = 'python3'

# If your documentation needs a minimal Sphinx version, state it here.
# needs_sphinx = '1.2'

# To perform a Sphinx version check that needs to be more specific than
# major.minor, call `check_sphinx_version("x.y.z")` here.
# check_sphinx_version("1.2.1")

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns.append('_templates')  # noqa: F405

# Templates path for custom layouts (separate from excluded RST templates)
templates_path = ['_templates']

# This is added to the end of RST files - a good place to put substitutions to
# be used globally.
rst_epilog += """
.. |icon-white-to-black| image:: /img/icons/white_to_black.png
  :scale: 80
  :alt: white to black icon

.. |icon-blink| image:: /img/icons/blink.png
  :scale: 80
  :alt: blink icon

.. |icon-plus| image:: /img/icons/picture_with_plus.png
  :scale: 40
  :alt: picture with a plus icon

.. |icon-plugins| image:: /img/icons/plugins.png
  :scale: 40
  :alt: plugins icon

.. |icon-settings-sliders| image:: /img/icons/settings_sliders.png
  :scale: 40
  :alt: settings sliders icon

.. |icon-zoom-pan-home| image:: /img/icons/zoom_pan_home.png
  :scale: 40
  :alt: reset zoom/pan icon

.. |icon-zoom-pan-2d| image:: /img/icons/zoom_pan_2d.png
  :scale: 40
  :alt: 2D zoom/pan icon

.. |icon-zoom-pan-horiz| image:: /img/icons/zoom_pan_horiz.png
  :scale: 40
  :alt: horizontal zoom/pan icon

.. |icon-zoom-pan-vert| image:: /img/icons/zoom_pan_vert.png
  :scale: 40
  :alt: vertical zoom/pan icon

.. |icon-region-horiz| image:: /img/icons/region_horiz.png
  :scale: 40
  :alt: horizontal region icon

.. |icon-region-circ| image:: /img/icons/region_circ.png
  :scale: 40
  :alt: circular region icon

.. |icon-eye| image:: /img/icons/eye.png
  :scale: 40
  :alt: eye icon

.. |icon-color-square| image:: /img/icons/color_square.png
  :scale: 40
  :alt: color square icon

.. |icon-box-zoom| image:: /img/icons/box_zoom.png
  :scale: 40
  :alt: box zoom icon

.. |icon-xrange-zoom| image:: /img/icons/xrange_zoom.png
  :scale: 40
  :alt: xrange zoom icon


.. |icon-line-select| image:: /img/icons/line_select.png
  :scale: 40
  :alt: line select icon


.. |icon-viewer-data-select| image:: /img/icons/viewer_data_select.png
  :scale: 30
  :alt: data select icon
"""  # noqa: F405

# -- Project information ------------------------------------------------------

# This does not *have* to match the package name, but typically does
project = "jdaviz"
author = "JDADF Developers"
copyright = '{0}, {1}'.format(
    datetime.datetime.now().year, author)

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.

# The full version, including alpha/beta/rc tags.
release = __version__
dev = "dev" in release
# The short X.Y version.
version = '.'.join(release.split('.')[:2])

extensions += ['sphinx.ext.extlinks', 'sphinx_design']  # noqa: F405

# get the most recent git commit hash at build time:
commit_hash = subprocess.run(
    # git call for the latest commit hash:
    'git rev-parse HEAD'.split(),
    capture_output=True
).stdout.decode().strip()

# inject the most recent commit hash into the github links used in `sample_notebooks.rst`:
extlinks = {
    'gh-tree': (
        f'https://github.com/spacetelescope/jdaviz/tree/{commit_hash}/%s', '%s'
    ),
    'gh-notebook': (
        f'https://github.com/spacetelescope/jdaviz/blob/{commit_hash}/notebooks/%s.ipynb', '%s notebook'
    )
}

# -- Options for HTML output --------------------------------------------------

# A NOTE ON HTML THEMES
# The global astropy configuration uses a custom theme, 'bootstrap-astropy',
# which is installed along with astropy. A different theme can be used or
# the options for this theme can be modified by overriding some of the
# variables set in the global configuration. The variables set in the
# global configuration are listed below, commented out.

html_static_path = ["_static"]
html_css_files = ["jdaviz.css"]
html_js_files = ["platform-context.js"]
html_copy_source = False

html_theme_options.update(  # noqa: F405
    {
        "github_url": "https://github.com/spacetelescope/jdaviz",
        "navbar_start": ["navbar-logo"],  # Keep logo
        # navbar_center will auto-populate from top-level toctree
        "navbar_end": ["theme-switcher", "navbar-icon-links"],  # Help Desk will be in icon links
        "icon_links": [
            {
                "name": "Help Desk",
                "url": "http://jwsthelp.stsci.edu/",
                "icon": "fa-solid fa-circle-question",
            }
        ],
        "use_edit_page_button": True,
        # https://github.com/pydata/pydata-sphinx-theme/issues/1492
        "navigation_with_keys": False,
    }
)

html_context = {
    "default_mode": "light",
    "to_be_indexed": ["stable", "latest"],
    "is_development": dev,
    "github_user": "spacetelescope",
    "github_repo": "jdaviz",
    "github_version": "main",
    "doc_path": "docs",
    "jdaviz_version": version if dev else release,  # Use short version for dev builds
}

# Add any paths that contain custom themes here, relative to this directory.
# To use a different custom theme, add the directory containing the theme.
# html_theme_path = []

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes. To override the custom theme, set this to the
# name of a builtin theme or the name of a custom theme in html_theme_path.

# html_theme = "sphinx_rtd_theme"

# Custom sidebar templates, maps document names to template names.
# html_sidebars = {}

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
html_logo = 'logos/jdaviz.svg'

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
html_favicon = 'logos/specviz2d.ico'

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
# html_last_updated_fmt = ''

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
html_title = '{0} v{1}'.format(project, release)

# Output file base name for HTML help builder.
htmlhelp_basename = project + 'doc'


# -- Options for LaTeX output -------------------------------------------------

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [('index', project + '.tex', project + u' Documentation',
                    author, 'manual')]


# -- Options for manual page output -------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [('index', project.lower(), project + u' Documentation',
              [author], 1)]


# -- Turn on nitpicky mode for sphinx (to warn about references not found) ----
nitpicky = True

# Do not populate this if you use nitpick-exceptions below.
nitpick_ignore = []

# Some warnings are impossible to suppress, and you can list specific references
# that should be ignored in a nitpick-exceptions file which should be inside
# the docs/ directory. The format of the file should be:
#
# <type> <class>
#
# for example:
#
# py:class astropy.io.votable.tree.Element
# py:class astropy.io.votable.tree.SimpleElement
# py:class astropy.io.votable.tree.SimpleElementWithContent
#
# Uncomment the following lines to enable the exceptions:
#
for line in open('nitpick-exceptions'):
    if line.strip() == "" or line.startswith("#"):
        continue
    dtype, target = line.split(None, 1)
    target = target.strip()
    nitpick_ignore.append((dtype, target))

# Extra intersphinx in addition to what is already in sphinx-astropy
intersphinx_mapping.update({  # noqa: F405
    'glueviz': ('https://docs.glueviz.org/en/stable/', None),
    'astroquery': ('https://astroquery.readthedocs.io/en/latest/', None),
    'glue': ('https://glue-core.readthedocs.io/en/latest/', None),
    'glue_jupyter': ('https://glue-jupyter.readthedocs.io/en/stable/', None),
    'photutils': ('https://photutils.readthedocs.io/en/stable/', None),
    'regions': ('https://astropy-regions.readthedocs.io/en/stable/', None),
    'roman_datamodels': ('https://roman-datamodels.readthedocs.io/en/stable/', None),
    'skimage': ('https://scikit-image.org/docs/stable/', None),
    'specreduce': ('https://specreduce.readthedocs.io/en/stable/', None),
    'specutils': ('https://specutils.readthedocs.io/en/stable/', None),
    'stdatamodels': ('https://stdatamodels.readthedocs.io/en/latest/', None),
    'traitlets': ('https://traitlets.readthedocs.io/en/stable/', None),
    'jwst': ('https://jwst-pipeline.readthedocs.io/en/stable/', None),
    'romancal': ('https://roman-pipeline.readthedocs.io/en/stable/', None),
})

# Options for linkcheck
linkcheck_ignore = [
    'https://github.com/spacetelescope/jdaviz/settings/branches',
    'https://pypi.org/project/jdaviz/#files'
]


# -- Grid items data for landing page -------------------------------------------

# Plugin data type mappings for filtering
plugin_data_types = {
    'aperture_photometry': ['image'],
    'catalog_search': ['image', 'catalog'],
    'collapse': ['3d'],
    'compass': ['image'],
    'cross_dispersion_profile': ['2d'],
    'data_quality': ['image', '1d', '2d', '3d'],
    'footprints': ['image'],
    'gaussian_smooth': ['1d', '2d', '3d'],
    'line_analysis': ['1d'],
    'line_lists': ['1d', '2d'],
    'line_profiles': ['3d'],
    'model_fitting': ['1d'],
    'moment_maps': ['3d'],
    'orientation': ['image'],
    'ramp_extraction': ['ramp'],
    'ramp_slice': ['ramp'],
    'slit_overlay': ['image', '2d'],
    'sonify': ['1d'],
    'spectral_extraction_2d': ['2d'],
    'spectral_extraction_3d': ['3d'],
    'spectral_slice': ['2d'],
}


def scan_directory_for_links(base_path, directory, data_type_map=None):
    """Scan a directory for RST files and return link information.

    Reads metadata from RST files in the format:
    :data-types: image 2d 3d
    :excl_platforms: desktop mast
    """
    links = []
    dir_path = os.path.join(base_path, directory)
    if not os.path.exists(dir_path):
        return links

    # Special case handling for acronyms in titles
    acronyms = {
        'vo': 'VO',
        'api': 'API',
        'url': 'URL',
    }

    for filename in sorted(os.listdir(dir_path)):
        if filename.endswith('.rst') and filename != 'index.rst' and filename != 'extensions.rst':
            # Convert filename to title (e.g., 'file_drop.rst' -> 'File Drop')
            name = filename[:-4].replace('_', ' ').title()

            # Replace known acronyms with proper capitalization
            name_words = name.split()
            for i, word in enumerate(name_words):
                word_lower = word.lower()
                if word_lower in acronyms:
                    name_words[i] = acronyms[word_lower]
            name = ' '.join(name_words)

            # Create relative path for Sphinx
            rel_path = os.path.join(directory, filename[:-4])

            link_data = {'text': name, 'href': rel_path}

            # Parse RST file for metadata
            rst_path = os.path.join(dir_path, filename)
            try:
                with open(rst_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # Look for field list metadata at the start of the file
                    for line in content.split('\n')[:20]:  # Check first 20 lines
                        line = line.strip()
                        if line.startswith(':data-types:'):
                            data_types = line.split(':', 2)[2].strip()
                            if data_types:
                                link_data['data_types'] = data_types
                        elif line.startswith(':excl_platforms:'):
                            excl_platforms = line.split(':', 2)[2].strip()
                            if excl_platforms:
                                link_data['excl_platforms'] = excl_platforms
            except Exception as e:
                print(f"Warning: Could not parse {rst_path}: {e}")

            # Fallback to data_type_map if provided and no metadata found
            if 'data_types' not in link_data and data_type_map and filename[:-4] in data_type_map:
                link_data['data_types'] = ' '.join(data_type_map[filename[:-4]])

            # For plugins directory, automatically extract data-types from Python code
            if 'data_types' not in link_data and directory == 'plugins':
                extracted_types = extract_data_types_from_plugin(
                    base_path, filename[:-4]
                )
                if extracted_types:
                    link_data['data_types'] = ' '.join(sorted(extracted_types))

            links.append(link_data)

    return links


def extract_data_types_from_plugin(base_path, plugin_name):
    """
    Extract data types from plugin Python code by analyzing dataset.filters.

    Returns a set of data type strings (e.g., {'1d', '2d', '3d', 'image', ...})
    """
    import re

    # Mapping from filter names to data types
    filter_to_datatype = {
        'is_flux_cube': '3d',
        'is_cube': '3d',
        'is_ramp_cube': 'ramp',
        'is_ramp_group_cube': 'ramp',
        'is_ramp_diff_cube': 'ramp',
        'is_ramp_integration': 'ramp',
        'is_spectrum': '1d',
        'is_spectrum_2d': '2d',
        'is_image': 'image',
        'is_catalog': 'catalog',
        'is_image_not_spectrum': 'image',
        'is_catalog_or_image_not_spectrum': ['catalog', 'image'],
    }

    generic_filters = {
        'is_not_wcs_only', 'not_child_layer', 'layer_in_viewers',
        'layer_is_not_dq', 'not_ramp', 'same_mosviz_row',
    }

    # Find the plugin Python file
    jdaviz_dir = os.path.abspath(os.path.join(base_path, '..', 'jdaviz'))
    configs = ['default', 'cubeviz', 'specviz', 'specviz2d',
               'mosviz', 'imviz', 'rampviz']

    python_files = []

    # Strategy 1: Exact match
    for config in configs:
        plugin_py_file = os.path.join(
            jdaviz_dir, 'configs', config, 'plugins',
            plugin_name, f'{plugin_name}.py'
        )
        if os.path.exists(plugin_py_file):
            python_files.append(plugin_py_file)

    # Strategy 2: Partial match
    if not python_files:
        for config in configs:
            plugins_dir = os.path.join(jdaviz_dir, 'configs', config, 'plugins')
            if os.path.exists(plugins_dir):
                for subdir in os.listdir(plugins_dir):
                    if (subdir in plugin_name or plugin_name in subdir):
                        subdir_path = os.path.join(plugins_dir, subdir)
                        if os.path.isdir(subdir_path):
                            for py_file in os.listdir(subdir_path):
                                if (py_file.endswith('.py') and
                                        not py_file.startswith('__')):
                                    python_files.append(
                                        os.path.join(subdir_path, py_file)
                                    )

    if not python_files:
        return set()

    # Parse the first matching file for filters
    try:
        with open(python_files[0], 'r', encoding='utf-8') as f:
            content = f.read()

        # Look for self.dataset.filters = [...]
        filter_match = re.search(
            r'self\.dataset\.filters\s*=\s*\[(.*?)\]',
            content, re.DOTALL
        )

        if not filter_match:
            return set()

        filter_names = re.findall(r"'([^']+)'", filter_match.group(1))

        data_types = set()
        for filter_name in filter_names:
            if filter_name in generic_filters:
                continue
            if filter_name in filter_to_datatype:
                dtype = filter_to_datatype[filter_name]
                if isinstance(dtype, list):
                    data_types.update(dtype)
                else:
                    data_types.add(dtype)

        return data_types

    except Exception:
        return set()


def check_extensions_exists(base_path, directory):
    """Check if extensions.rst exists in the directory."""
    extensions_path = os.path.join(base_path, directory, 'extensions.rst')
    if os.path.exists(extensions_path):
        return os.path.join(directory, 'extensions')
    return None



# Unified descriptions for grid items and wireframe sidebars
descriptions = {
    'loaders': 'Import data of multiple formats and from multiple sources into jdaviz',
    'plugins': 'Do basic data reduction and analysis tasks for specific science use-cases',
    'viewers': ('Show data in a variety of different viewers '
                'custom built for exploring astronomical data'),
    'subsets': ('Select regions of interest in your data, see that synced across all '
                'viewers, and use as inputs to data analysis tasks'),
    'export': 'Export generated data, selected subsets, and viewers',
    'settings': 'Choose how to visualize your data',
    'settings_plot': ('Customize viewer appearance including axes labels, limits, stretching, '
                      'color maps, markers, and display options.'),
    'settings_units': ('Convert and display data in different unit systems. Choose spectral '
                       'units (wavelength, frequency, energy) and flux units appropriate for '
                       'your analysis.'),
    'info': 'Interactive access to information about your data and generated results',
    'info_metadata': ('View FITS header information, WCS coordinates, and other metadata for '
                      'loaded datasets.'),
    'info_markers': ('Interactively create markers in any viewer and information about the '
                     'underlying data will be exposed and available to export into the notebook'),
    'info_logger': ('System messages, warnings, and operation history. Monitor plugin execution, '
                    'data loading status, and any issues that arise during analysis.'),
    'userapi': 'Script advanced and reproducible workflows in the notebook mimicing UI-operations',
    'data_menu': 'Control data and subset layer order and visibility for each viewer',
    'mouseover': 'See information about the data directly below your cursor',
}


# Build grid items structure
docs_dir = os.path.dirname(__file__)

# Data type mappings for loaders/formats
loader_formats_data_types = {
    '1d_spectrum': ['1d'],
    '2d_spectrum': ['2d'],
    '3d_spectrum': ['3d'],
    'catalog': ['catalog'],
    'image': ['image'],
    'ramp': ['ramp']
}

# Data type mappings for viewers
viewer_data_types = {
    'spectrum_1d': ['1d'],
    'profile_1d': ['1d', '2d', '3d'],
    'spectrum_2d': ['2d'],
    'image_2d': ['image', '2d', '3d'],
    'table': ['catalog'],
    'histogram': ['1d', 'image', '2d', '3d'],
    'scatter': ['catalog']
}

grid_items_data = [
    {
        'title': 'Data Loaders',
        'description': descriptions['loaders'],
        'icon': 'mdi-plus-box',
        'grid_id': 'grid-loaders',
        'two_column': True,
        'column_headers': ['Sources:', 'Formats:'],
        'links': [
            scan_directory_for_links(docs_dir, 'loaders/sources'),
            scan_directory_for_links(docs_dir, 'loaders/formats', loader_formats_data_types)
        ],
        'extensions_path': check_extensions_exists(docs_dir, 'loaders')
    },
    {
        'title': 'Data Analysis Plugins',
        'description': descriptions['plugins'],
        'icon': 'mdi-tune-variant',
        'grid_id': 'grid-plugins',
        'filters': [
            {'name': 'All', 'id': 'all', 'active': True},
            {'name': 'Images', 'id': 'image'},
            {'name': '1D Spectra', 'id': '1d'},
            {'name': '2D Spectra', 'id': '2d'},
            {'name': '3D Spectra', 'id': '3d'},
            {'name': 'Catalog', 'id': 'catalog'},
            {'name': 'Ramp', 'id': 'ramp'}
        ],
        'links': scan_directory_for_links(docs_dir, 'plugins', plugin_data_types),
        'extensions_path': check_extensions_exists(docs_dir, 'plugins')
    },
    {
        'title': 'Viewers',
        'description': descriptions['viewers'],
        'icon': 'mdi-plus-box',
        'grid_id': 'grid-viewers',
        'links': scan_directory_for_links(docs_dir, 'viewers', viewer_data_types),
        'extensions_path': check_extensions_exists(docs_dir, 'viewers')
    },
    {
        'title': 'Subsets',
        'description': descriptions['subsets'],
        'icon': 'mdi-selection',
        'grid_id': 'grid-subsets',
        'links': scan_directory_for_links(docs_dir, 'subsets'),
        'extensions_path': check_extensions_exists(docs_dir, 'subsets')
    },
    {
        'title': 'Export',
        'description': descriptions['export'],
        'icon': 'mdi-content-save',
        'grid_id': 'grid-export',
        'links': scan_directory_for_links(docs_dir, 'export')
    },
    {
        'title': 'Flexible Settings & Options',
        'description': descriptions['settings'],
        'icon': 'mdi-cog',
        'grid_id': 'grid-settings',
        'links': scan_directory_for_links(docs_dir, 'settings')
    },
    {
        'title': 'Access to Data Info',
        'description': descriptions['info'],
        'icon': 'mdi-information-outline',
        'grid_id': 'grid-info',
        'links': scan_directory_for_links(docs_dir, 'info')
    },
    {
        'title': 'Data Menu',
        'description': descriptions['data_menu'],
        'icon': 'mdi-alpha-a-box-outline',
        'grid_id': 'grid-data-menu',
        'links': scan_directory_for_links(docs_dir, 'data_menu')
    },
    {
        'title': 'Mouseover Information',
        'description': descriptions['mouseover'],
        'icon': 'mdi-auto-fix',
        'grid_id': 'grid-mouseover',
        'links': scan_directory_for_links(docs_dir, 'mouseover')
    },
    {
        'title': 'API Access in Notebook',
        'description': descriptions['userapi'],
        'icon': 'api',
        'grid_id': 'grid-userapi',
        'links': scan_directory_for_links(docs_dir, 'userapi')
    }
]

# Make grid items available to templates
if 'html_context' not in locals():
    html_context = {}
html_context['grid_items'] = grid_items_data
html_context['descriptions'] = descriptions


# -- Custom directive -------------------------------------------

class PluginAvailabilityDirective(SphinxDirective):
    """
    Automatically generate plugin availability information from Python code.

    Reads the plugin's __init__ to extract:
    - observe_traitlets_for_relevancy
    - dataset.filters
    - viewer filters

    Generates a banner showing when the plugin is available/visible.
    """

    def run(self):
        # Get the current document's name (e.g., 'plugins/spectral_extraction_3d')
        doc_name = self.env.docname

        # Extract plugin name from doc path
        if not doc_name.startswith('plugins/'):
            error_node = nodes.warning(
                '',
                nodes.paragraph(text='This directive can only be used in plugin documentation.')
            )
            return [error_node]

        plugin_name = doc_name.split('/')[-1]  # e.g., 'spectral_extraction_3d'

        # Find the plugin Python file
        jdaviz_dir = os.path.abspath(os.path.join(self.env.srcdir, '..', 'jdaviz'))
        python_file = self._find_plugin_file(jdaviz_dir, plugin_name)

        if not python_file:
            # No plugin file found - skip silently
            return []

        # Parse the plugin file
        try:
            with open(python_file, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            error_node = nodes.warning(
                '',
                nodes.paragraph(text=f'Could not read plugin file: {e}')
            )
            return [error_node]

        # Extract information
        data_types = self._extract_data_types(content)
        relevancy_info = self._extract_relevancy(content)

        # Generate availability text
        availability_parts = []

        if data_types:
            dtype_names = {
                '1d': '1D spectra',
                '2d': '2D spectra',
                '3d': '3D spectral cubes',
                'image': 'images',
                'catalog': 'catalogs',
                'ramp': 'ramp data'
            }
            dtype_list = [dtype_names.get(dt, dt) for dt in sorted(data_types)]
            if len(dtype_list) == 1:
                availability_parts.append(f"This plugin works with **{dtype_list[0]}**.")
            else:
                availability_parts.append(
                    f"This plugin works with **{', '.join(dtype_list[:-1])} "
                    f"and {dtype_list[-1]}**."
                )

        if relevancy_info:
            if relevancy_info['type'] == 'dataset_items':
                availability_parts.append(
                    "The plugin will be visible when at least one compatible dataset "
                    "is loaded."
                )
            elif relevancy_info['type'] == 'viewer_items':
                availability_parts.append(
                    "The plugin will be visible when at least one viewer is available."
                )
            elif relevancy_info['type'] == 'trace_dataset_items':
                availability_parts.append(
                    "The plugin will be visible when trace data is loaded."
                )

        if not availability_parts:
            # No specific information found
            return []

        # Create an admonition node
        admonition_node = nodes.admonition()
        admonition_node += nodes.title(text='Plugin Availability')
        admonition_node['classes'].append('note')

        for part in availability_parts:
            # Parse the reStructuredText to handle inline markup
            paragraph = nodes.paragraph()
            # Use string list for nested_parse
            from docutils.statemachine import StringList
            text_list = StringList([part], source='<plugin-availability>')
            self.state.nested_parse(text_list, 0, paragraph)
            admonition_node += paragraph

        return [admonition_node]

    def _find_plugin_file(self, jdaviz_dir, plugin_name):
        """Find the Python file for a plugin."""
        configs = ['default', 'cubeviz', 'specviz', 'specviz2d',
                   'mosviz', 'imviz', 'rampviz']

        # Strategy 1: Exact match
        for config in configs:
            plugin_py_file = os.path.join(
                jdaviz_dir, 'configs', config, 'plugins',
                plugin_name, f'{plugin_name}.py'
            )
            if os.path.exists(plugin_py_file):
                return plugin_py_file

        # Strategy 2: Partial match (e.g., spectral_extraction_3d -> spectral_extraction)
        for config in configs:
            plugins_dir = os.path.join(jdaviz_dir, 'configs', config, 'plugins')
            if os.path.exists(plugins_dir):
                for subdir in os.listdir(plugins_dir):
                    if subdir in plugin_name or plugin_name in subdir:
                        for py_file in os.listdir(os.path.join(plugins_dir, subdir)):
                            if py_file.endswith('.py') and not py_file.startswith('__'):
                                full_path = os.path.join(plugins_dir, subdir, py_file)
                                return full_path

        return None

    def _extract_data_types(self, content):
        """Extract data types from dataset.filters."""
        import re

        # Mapping from filter names to data types
        filter_to_datatype = {
            'is_flux_cube': '3d',
            'is_cube': '3d',
            'is_ramp_cube': 'ramp',
            'is_ramp_group_cube': 'ramp',
            'is_ramp_diff_cube': 'ramp',
            'is_ramp_integration': 'ramp',
            'is_spectrum': '1d',
            'is_spectrum_2d': '2d',
            'is_image': 'image',
            'is_catalog': 'catalog',
            'is_image_not_spectrum': 'image',
            'is_catalog_or_image_not_spectrum': ['catalog', 'image'],
        }

        generic_filters = {
            'is_not_wcs_only', 'not_child_layer', 'layer_in_viewers',
            'layer_is_not_dq', 'not_ramp', 'same_mosviz_row',
        }

        # Look for self.dataset.filters = [...]
        filter_match = re.search(
            r'self\.dataset\.filters\s*=\s*\[(.*?)\]',
            content, re.DOTALL
        )

        if not filter_match:
            return set()

        filter_names = re.findall(r"'([^']+)'", filter_match.group(1))

        data_types = set()
        for filter_name in filter_names:
            if filter_name in generic_filters:
                continue
            if filter_name in filter_to_datatype:
                dtype = filter_to_datatype[filter_name]
                if isinstance(dtype, list):
                    data_types.update(dtype)
                else:
                    data_types.add(dtype)

        return data_types

    def _extract_relevancy(self, content):
        """Extract relevancy observation info."""
        import re

        # Look for observe_traitlets_for_relevancy
        relevancy_match = re.search(
            r'observe_traitlets_for_relevancy\s*\(\s*traitlets_to_observe\s*=\s*\[\'([^\']+)\'\]',
            content
        )

        if relevancy_match:
            traitlet = relevancy_match.group(1)
            return {'type': traitlet}

        return None


class JdavizCLIHelpDirective(SphinxDirective):

    def run(self):
        help_text = subprocess.check_output(["jdaviz", "--help"], encoding="utf-8")
        paragraph_node = nodes.literal_block(text=help_text)
        return [paragraph_node]


class JdavizLandingPageDirective(SphinxDirective):
    """Render the landing page template with Jinja2 processing."""

    def run(self):
        import jinja2

        # Get the template directory
        template_dir = os.path.join(self.env.srcdir, '_templates')

        # Create a Jinja2 environment
        jinja_env = jinja2.Environment(loader=jinja2.FileSystemLoader(template_dir))

        # Add pathto function to the environment
        def pathto(otheruri, resource=False):
            """Generate relative path to another document."""
            if resource:
                return otheruri
            if not otheruri.endswith('.html') and not otheruri.startswith('#'):
                otheruri = otheruri + '.html'
            return otheruri

        # Load and render the template with context
        template = jinja_env.get_template('index.html')

        # Get html_context from the Sphinx app config
        app_html_context = self.env.app.config.html_context

        context = {
            'grid_items': app_html_context.get('grid_items', []),
            'jdaviz_version': app_html_context.get('jdaviz_version', ''),
            'descriptions': app_html_context.get('descriptions', {}),
            'pathto': pathto
        }
        html_content = template.render(context)

        # Create raw HTML node
        raw_node = nodes.raw('', html_content, format='html')
        return [raw_node]


class PluginApiReferencesDirective(SphinxDirective):
    """
    Auto-generate API reference links for plugin exposed attributes.

    Usage:
        .. plugin-api-refs::
           :module: jdaviz.configs.default.plugins.collapse.collapse
           :class: Collapse
    """
    required_arguments = 0
    optional_arguments = 0
    option_spec = {
        'module': str,
        'class': str,
    }

    def run(self):
        module_path = self.options.get('module')
        class_name = self.options.get('class')

        if not module_path or not class_name:
            error_node = nodes.error()
            error_node += nodes.paragraph(
                text='plugin-api-refs directive requires :module: and :class: options'
            )
            return [error_node]

        try:
            # Import the module and get the class
            import importlib
            module = importlib.import_module(module_path)
            plugin_class = getattr(module, class_name)

            # Create a temporary instance to access user_api
            # We need to mock the required attributes for plugin initialization
            class MockApp:
                hub = None

                def __init__(self):
                    from unittest.mock import MagicMock
                    self.hub = MagicMock()

            # Get the class docstring and process it
            rst_lines = [
                '',
                'API References',
                '--------------',
                '',
            ]

            # Extract exposed attributes to validate documentation coverage
            exposed_attrs = []
            try:
                # Try to get the exposed attributes from user_api
                from unittest.mock import MagicMock
                mock_app = MagicMock()
                mock_app.hub = MagicMock()
                instance = plugin_class(app=mock_app)
                user_api = instance.user_api
                exposed_attrs = list(user_api._expose)
            except Exception:
                # If instantiation fails, parse source to get exposed list
                import inspect
                try:
                    source = inspect.getsource(plugin_class.user_api.fget)
                    import re
                    match = re.search(
                        r"expose\s*=\s*[\(\[]([^\)\]]+)[\)\]]",
                        source,
                        re.DOTALL
                    )
                    if match:
                        expose_str = match.group(1)
                        exposed_attrs = re.findall(r"['\"]([^'\"]+)['\"]", expose_str)
                except Exception:
                    exposed_attrs = []

            if plugin_class.__doc__:
                docstring = plugin_class.__doc__.strip()

                # Remove the intro paragraph that references the current page
                # This typically ends with "for more details."
                lines = docstring.split('\n')
                filtered_lines = []
                skip_intro = False

                for i, line in enumerate(lines):
                    stripped = line.strip()
                    # Check if this line contains a self-reference to the plugin page
                    if ':ref:`' in stripped and 'for more details' in stripped.lower():
                        skip_intro = True
                        continue
                    # Skip the sentence before the self-reference too
                    next_line = lines[i+1] if i < len(lines) - 1 else ''
                    if ':ref:`' in next_line and 'for more details' in next_line.lower():
                        skip_intro = True
                        continue
                    # Once we hit the "Only the following" section, stop skipping
                    starts_list = stripped.startswith('*')
                    if skip_intro and ('Only the following' in stripped or starts_list):
                        skip_intro = False

                    if not skip_intro:
                        filtered_lines.append(line)

                # Join and clean up
                docstring_content = '\n'.join(filtered_lines).strip()

                # Expand short-form Sphinx references to full paths
                # This converts :meth:`method_name` to :meth:`~module.Class.method_name`
                if docstring_content:
                    import re

                    # Pattern for short-form :meth: without module path
                    # Matches :meth:`name` but not :meth:`~path.name` or :meth:`path.name`
                    def expand_meth_ref(match):
                        method_name = match.group(1)
                        # If already has a path, leave it alone
                        if '.' in method_name or method_name.startswith('~'):
                            return match.group(0)
                        # Otherwise, expand to full path with ~ to show just the method name
                        return f':meth:`~{module_path}.{class_name}.{method_name}`'

                    docstring_content = re.sub(
                        r':meth:`([^`]+)`',
                        expand_meth_ref,
                        docstring_content
                    )

                    # Pattern for short-form :attr: without module path
                    def expand_attr_ref(match):
                        attr_name = match.group(1)
                        if '.' in attr_name or attr_name.startswith('~'):
                            return match.group(0)
                        return f':attr:`~{module_path}.{class_name}.{attr_name}`'

                    docstring_content = re.sub(
                        r':attr:`([^`]+)`',
                        expand_attr_ref,
                        docstring_content
                    )

                # Validate that all exposed attributes are documented
                if exposed_attrs and docstring_content:
                    import re
                    # Find all documented attributes (methods and properties)
                    # Look for :meth:`method_name` or ``attr_name``
                    documented = set()
                    # Match :meth:`name` or :meth:`~path.name`
                    for match in re.finditer(r':meth:`[~]?(?:[^.`]+\.)*([^`]+)`',
                                             docstring_content):
                        documented.add(match.group(1))
                    # Match ``attr_name`` patterns
                    for match in re.finditer(r'``([a-z_][a-z0-9_]*)``', docstring_content):
                        documented.add(match.group(1))

                    # Check for undocumented attributes
                    undocumented = [
                        attr for attr in exposed_attrs if attr not in documented
                    ]

                    if undocumented:
                        from sphinx.util import logging
                        logger = logging.getLogger(__name__)
                        logger.warning(
                            f'{class_name}: The following user API attributes are not '
                            f'documented in the class docstring: {", ".join(undocumented)}',
                            location=(self.env.docname, self.lineno)
                        )

                # Add the docstring content
                if docstring_content:
                    rst_lines.append(docstring_content)
                    rst_lines.append('')
                else:
                    rst_lines.append(
                        'This plugin is primarily UI-driven. '
                        'See the :ref:`plugin-apis` documentation for general '
                        'plugin API methods.'
                    )
                    rst_lines.append('')
            else:
                rst_lines.append(
                    'This plugin is primarily UI-driven. '
                    'See the :ref:`plugin-apis` documentation for general '
                    'plugin API methods.'
                )
                rst_lines.append('')

            # Add a simple reference to the full class documentation
            # Use full module path with ~ to show just the class name
            rst_lines.append(
                f'For detailed API documentation, see :class:`~{module_path}.{class_name}`.'
            )
            rst_lines.append('')

            # Parse the RST and return nodes
            from docutils.parsers.rst import Parser
            from docutils.utils import new_document

            rst_text = '\n'.join(rst_lines)
            parser = Parser()
            doc = new_document('<plugin-api-refs>')
            doc.settings = self.state.document.settings
            parser.parse(rst_text, doc)

            return doc.children

        except Exception as e:
            # Return an error node if something goes wrong
            error_node = nodes.error()
            error_node += nodes.paragraph(
                text=f'Error generating plugin API references: {str(e)}'
            )
            return [error_node]


def setup(app):
    app.add_directive('jdavizclihelp', JdavizCLIHelpDirective)
    app.add_directive('jdavizlanding', JdavizLandingPageDirective)
    app.add_directive('plugin-api-refs', PluginApiReferencesDirective)
    app.add_directive('plugin-availability', PluginAvailabilityDirective)
